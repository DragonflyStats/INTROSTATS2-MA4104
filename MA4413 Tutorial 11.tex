

\section*{Question 1}
\begin{frame}
A input source is a random variable X with a four letter alphabet $\{A,B,C,D\}$.
There are four different probability distributions presented below. 
Compute the entropy for each case.

\begin{tabular}{|c|c|c|c|c|}
\hline
	&	Case 1	&	Case 2	&	Case 3	&	Case 4	\\	
Xi	&	P(Xi)	&	P(Xi)	&	P(Xi)	&	P(Xi)	\\	\hline
A	&	0.25	&	0.25	&	0.7	&	0.97	\\	\hline
B	&	0.25	&	0.5	&	0.1	&	0.01	\\	\hline
C	&	0.25	&	0.125	&	0.1	&	0.01	\\	\hline
D	&	0.25	&	0.125	&	0.1	&	0.01	\\	\hline
\end{tabular} 

\end{frame}
\section{Question 2}
\begin{frame}
The input source to a noisy communication channel is a random variable X over three symbols a,b,c. The output from this channel is a random variable Y over the same three symbols. The joint distribution of the these two random variables is as follows:

\begin{tabular}{|c|c|c|c|}
\hline	&	y=a	&	y=b	&	y=c	\\	\hline
x=a	&	0.25	&	0	&	0.125	\\	\hline
x=b	&	0	&	0.125	&	0.25	\\	\hline
x=c	&	0.125	&	0	&	0.125	\\	\hline
\end{tabular} 
\end{frame}
\begin{frame}
\begin{itemize}
\item Write down the marginal distributions for X and Y.

\item Compute the marginal entropies $H(X)$ and $H(Y)$

\item Compute the joint entropy $H(X,Y)$ of the two random variables.
\end{itemize}
%---------------------------------------------------- %
\section*{Question 3a}
A four letter alphabet is encoded into binary form according to
\begin{tabular}{|c|c|c|c|c|}
\hline
Case 1	&	A:  10 	&	C:  110	&	G:  111	& T:  0 \\ \hline
Case 2	&	A:  00	&	C:  01	&		G: 10	&	T: 11 \\ \hline
\end{tabular} 
	

Using the code presented in case 1, decode the following sequence:	
\[11110001011010\]
Encode this message using the code from case 2. Compare the length of messages in both cases.

\section*{Question 3a}
Given that the alphabet has the following distribution 
\begin{tabular}{|c|c|c|c|c|}
\hline
Xi & A & C & G & T \\ \hline
P(Xi) & 0.25 & 0.125 & 0.125 & 0. 5 \\
\hline
\end{tabular} 

Compute the average symbol length for both cases.

\end{document}