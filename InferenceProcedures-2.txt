\section{Inference procedures}

%---------------------------------------------------------%
t.test
prop.test
Anderson darling test
Wilcoxon.text
Simple.median.test
Shapiro.test

%---------------------------------------------------------%

Confidence Interval Estimation
In statistics one often would like to estimate unknown parameters for a known distribution. For example, you may think that your parent population is normal, but the mean is unknown, or both the mean and standard deviation are unknown. From a data set you can't hope to know the exact values of the parameters, but the data should give you a good idea what they are. For the mean, we expect that the sample mean or average of our data will be a good choice for the population mean, and intuitively, we understand that the more data we have the better this should be. How do we quantify this?

Statistical theory is based on knowing the sampling distribution of some statistic such as the mean. This allows us to make probability statements about the value of the parameters, such as we are 95 per cent certain the parameter is in some range of values.

In this section, we describe the R functions prop.test, t.test, and wilcox.test used to facilitate the calculations.


%---------------------------------------------------------%


Notice, in particular, we get the 95% confidence interval (0:32; 0:52) by default.

If we want a 90% confidence interval we need to ask for it:

> prop.test(42,100,conf.level=0.90)
1-sample proportions test with continuity correction
data: 42 out of 100, null probability 0.5
X-squared = 2.25, df = 1, p-value = 0.1336
alternative hypothesis: true p is not equal to 0.5
90 percent confidence interval:
0.3372368 0.5072341
sample estimates:
p
0.42

%---------------------------------------------------------%

Which gives the interval (0:33; 0:50). Notice this is smaller as we are now less confident.

Con_dence interval for the median
Con_dence intervals for the median are important too. They are di_erent mathematically than the ones above,
but in R these di_erences aren't noticed. The R function wilcox.test performs a non-parametric test for the median.
Suppose the following data is pay of CEO's in America in 2001 dollars11, then the following creates a test for the
median
> x = c(110, 12, 2.5, 98, 1017, 540, 54, 4.3, 150, 432)
> wilcox.test(x,conf.int=TRUE)
Wilcoxon signed rank test
data: x
V = 55, p-value = 0.001953
alternative hypothesis: true mu is not equal to 0
95 percent confidence interval:
33.0 514.5

%---------------------------------------------------------%
\begin{frame}
\frametitle{Tests for the median}
Suppose a study of cell-phone usage for a user gives the following lengths for the calls
12:8 3:5 2:9 9:4 8:7 :7 :2 2:8 1:9 2:8 3:1 15:8
What is an appropriate test for center?
First, look at a stem and leaf plot
x = c(12.8,3.5,2.9,9.4,8.7,.7,.2,2.8,1.9,2.8,3.1,15.8)
> stem(x)
...
0 | 01233334
0 | 99
1 | 3
1 | 6

\end{frame}
%---------------------------------------------------------%
\begin{frame}
\frametitle{Tests for the median}
The distribution looks skewed with a possibly heavy tail. A t-test is ruled out. Instead, a test for the median is done.
Suppose H0 is that the median is 5, and the alternative is the median is bigger than 5. To test this with R we can
use the wilcox.test as follows

> wilcox.test(x,mu=5,alt="greater")
Wilcoxon signed rank test with continuity correction
data: x
V = 39, p-value = 0.5156
alternative hypothesis: true mu is greater than 5
Warning message:
Cannot compute exact p-value with ties ...
Note the p value is not small, so the null hypothesis is not rejected.
\end{frame}
%---------------------------------------------------------%
\begin{frame}
\frametitle{Tests for the median}
Simple median test
A function to do so is simple.median.test. This computes the p-value for a two-sided test for a specified median. To see it work, we have

> x = c(12.8,3.5,2.9,9.4,8.7,.7,.2,2.8,1.9,2.8,3.1,15.8)
> simple.median.test(x,median=5)
[1] 0.3876953 # accept
> simple.median.test(x,median=10)
[1] 0.03857422 # reject

\end{frame}
%---------------------------------------------------------%
\begin{frame}
\frametitle{Chi-squared Test of Independence}

The chi-squared distribution allows for statistical tests of categorical data. Among these tests are those for goodness of fit and independence.

\end{frame}
%---------------------------------------------------------%
\begin{frame}
\frametitle{Chi-squared Test of Independence}

Two random variables x and y are called independent if the probability distribution of one variable is not affected by the presence of another. 
Assume fij is the observed frequency count of events belonging to both i-th category of x and j-th category of y. Also assume eij to be the corresponding expected count if x and y are independent. The null hypothesis of the independence assumption is to be rejected if the p-value of the following Chi-squared test statistics is less than a given significance level a. 

\end{frame}
%---------------------------------------------------------% 
Example
In the built-in data set survey, the Smoke column records the students smoking habit, while the Exer column records their exercise level. The allowed values in Smoke are "Heavy", "Regul" (regularly), "Occas" (occasionally) and "Never". As for Exer, they are "Freq" (frequently), "Some" and "None". 
We can tally the students smoking habit against the exercise level with the table function in R. The result is called the contingency table of the two variables. 
\begin{frame}
> library(MASS)       # load the MASS package 
> tbl = table(survey$Smoke, survey$Exer) 
> tbl                 # the contingency table 
 
        Freq None Some 
  Heavy    7    1    3 
  Never   87   18   84 
  Occas   12    3    4 
  Regul    9    1    7 

\end{frame}
%---------------------------------------------------------% 

Problem
Test the hypothesis whether the students smoking habit is independent of their exercise level at .05 significance level. 
Solution
We apply the chisq.test function to the contingency table tbl, and found the p-value to be 0.4828. 
> chisq.test(tbl) 
 
        Pearson’s Chi-squared test 
 
data:  table(survey$Smoke, survey$Exer) 
X-squared = 5.4885, df = 6, p-value = 0.4828 
 
Warning message: 
In chisq.test(table(survey$Smoke, survey$Exer)) : 
  Chi-squared approximation may be incorrect 
Answer


\end{frame}
%---------------------------------------------------------% 
As the p-value 0.4828 is greater than the .05 significance level, we do not reject the null hypothesis that the smoking habit is independent of the exercise level of the students. 
Enhanced Solution
The warning message found in the solution above is due to the small cell values in the contingency table. To avoid such warning, we combine the second and third columns of tbl, and save it in a new table named ctbl. Then we apply the chisq.test function against ctbl instead. 
> ctbl = cbind(tbl[,"Freq"], tbl[,"None"] + tbl[,"Some"]) 
> ctbl 
      [,1] [,2] 
Heavy    7    4 
Never   87  102 
Occas   12    7 
Regul    9    8 
 
\end{frame}
%---------------------------------------------------------% 
> chisq.test(ctbl) 
 
        Pearson’s Chi-squared test 
 
data:  ctbl 
X-squared = 3.2328, df = 3, p-value = 0.3571 


%---------------------------------------------------------%