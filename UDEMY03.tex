
%=======================================================================================%
Chapter 3: DISCRETE RANDOM VARIABLES
IN Tats Cl-IAPTERI

\begin{enumerate}
\item Random Variables
\item Discrete Probability Distribution
\item Distribution Functions for
Random Variables
\item Distribution Functions for Discrete
Random Variables
\item Expected Values
\item Variance and Standard Deviation
\item Some Theorems on Expectation
\item Some Theorems on Variance
\end{enumerate}

\newpage
\subsection*{Random Variables}
Suppose that to each point of a sample space We assign a number, We
then have aﬁmction deﬁned on the sample space. This function is called
a random variable (or stochastic variable) or more precisely. a random
2 3
% %--- Copyright 2001 by the McGtaW-Hill Companies, Inc. Click Here for Tenns of Use.



24 PROBABILITY AND STATISTICS
ﬁmctiuri (stochastic function). It is usually
denoted by a capital letter such as X or Y. In %
general, a random variable has some speci- ‘Z *
ﬁed physical, geometrical, or other signiﬁ-
cance.

A random variable that takes on a ﬁnite or countably inﬁnite number of values is
called a discrete random variable while one that takes on a noncountably inﬁnite number
of values is called a nundiscrete random variable.

%=============================================================================================================== %
\subsection*{Discrete Probability Distribution}
Let X be a discrete random variable, and suppose that the possible val-
ues that it can assume are given by xi, x2, xi,  , arranged in some order.
Suppose also that these values are assumed with probabilities given by
\[P(X=xk)=f(xk) k:l,2‘n;| \](1)
It is convenient to introduce the probabmtyfimction, also referred
to as probability distribution, given by
\[P(X=X)=f(X) \](2)
For x =xk , this reduces to our previous equation, while for other
values of x,f(x) = 0.
In general, f(x) is a probability function if
1. f(x)20
2. 2f(X)=l
.v



CHAPTER 3; Discrete Random Variables 25
where the sum in the second property above is taken over all possible
values of x.
%=============================================================================================================== %
\subsection*{Example 3.1.}
 Suppose that a coin is tossed twice, Let X represent
the number of heads that can come up. With each sample point we can
associate a number for X as follows:

% % Table 
Sample Point I HH HT TH TT
X I 2 1 1 0
Now we can ﬁnd the probability function corresponding to
the random variable X. Assuming the coin is fair, we have
l l 1 1
PHH =— PHT=— PTH =— PTI"=—
()4()4()4()4
Then
P(X=0)=P(TT)=%
l l 1
P(X=1)=P(HTuTH):P(HT)+P(TH):Z+Z=5
P(X=2)=P(HH)=%
Thus, the probability function is given by
IQ
%============================================================================================================== %
\subsection*{Distribution Functions for Random Variables}
The cumulative distribution function, or brieﬂy the distribution func-
tiun, for a random variable X is deﬁned by



26 PROBABILITY AND STATISTICS
F(X) = P(X s X) (3)
Where x is any real number, iie,, —<><> S x S ><>.
In Words, the cumulative distribution function will determine
the probability that the random variable will take on any value x or less.
The distribution function F (x) has the following properties:
1. F(x) is nondecreasing U467, F(x) S F(y) ifx S y].
2. lim F(x) = 0; lim F(x) =1
¢\*>%“ ¢\'*}*><'
3. F(x) is continuous from the right [i.e., lim_ F(x +h) : F(x)
for all x]. ‘HO

%=======================================================================================%
Distribution Functions for Discrete Random Variables

The distribution function for a discrete random variable X can be
obtained from its probability function by noting that, for all x in (-<><>,w),
O —<><><X<x,
f(/Y1) X157‘-<XZ
F(X)= f(x|)+f(x2) Xz5x<X3
f(X|)+"-f(X,,) X~5X<°° (4)
It is clear that the probability function of zt discrete random variable
can be obtained from the distribution function noting that
f(K) = F(X)—“1iI11, F(u) (5)

%======================================================================================================================== %
$
% % CHAPTER 3: Discrete Random Variables 27

\subsection*{Expected Values}
A very important concept in probability and statistics is that of mathe-
matical expectation, expected value, or brieﬂy the expectation, of a ran-
dom variable. For a discrete random variable X having the possible val-
ues xl, x2, ..., m, the expectation ofX is deﬁned as
\[ E<x)=X,P(x=x,>+~-+x,,P<x=X,)=ﬁxjP(x=i,) \] % % (6)
/:1
or equivalently, if P(x = xj) = f(x]-) ,
E<X> = »<.f<m+--~+m¢'<»:,,> = ix are V) = Xx/(X) <7)
J J
/:1 X
where the last summation is taken over all appropriate values of x.
Notice that when the probabilities are all equal,
E(X) :  (3)
n
Which is simply the mean ofxl, 1:2, .,., m .
%=============================================================================================================== %

\subsubsection*{Example 3.2.} 
Suppose that a game is to be played with a single die
assumed fair. In this game a player wins \$20 if a 2 turns up; \$40 if a 4
turns up; loses \$30 if a 6 turns up; while the player neither wins nor
loses if any other face turns up. 
%=============================================================================================================== %

Find the expected sum of money to be won.

Let X be the random variable giving the amount of money won on
any toss. The possible amounts won when the die turns up 1, 2,  6
are xi, x2, ,.., x5, respectively, While the probabilities of these are f(x]),
f(xZ), ...,f(x6). The probability function for X is given by:


%=======================================================================================%

28 PROBABILITY AND STATISTICS
ix | 0 i+2oi 0 i+40i 0 i -aoi
\fm| 1/ell/6 l 1/6 l 1/at 1/at 1/sl
Therefore, the expected value, or expectation, is
EmIwe+as+<@><a+~@>e>+<@»e>Hate)15
It follows that the player can expect to win $5. In a fair
game, therefore, the player should expect to pay $5 in order to play
the game,
I .
Remember M‘
II
The expected value of a discrete ran- "'
dom variable is its measure of central
tendency! “""'

%============================================================================================================================================== %

\subsection*{Variance and Standard Deviation}
We have already noted that the expectation of a random variable X is
often called the mean and can be denoted by ,u. 

As we noted in Chapter Two, another quantity of great importance in probability and statistics is the variance. 
If X is a discrete random variable taking the values x’, x2,
m, and having probability function ﬁx), then the variance is given
by
6% = E[<X—#)2] = Ztx,» —m2f<x,> = Zo —u>’f<x> <9>
/=1 t
In the special case where all the probabilities are equal, we have

%%-CHAPTER 3; Discrete Random Variables 29

_ 2 _ 2  _ 2
o.§=(Xl 11> +<><2 11> + +<»<,, 11> (10)
Fl
which is the variance We found for a set of n numbers values xi, x2,
, x .
V!

Example 3.3. Find the variance for the game played in Example
3.2,

Recall the probability function for the game:
ix]. to l+20l 0 l+40\0 l-sol
lf(x].)l1/ﬁll/6l1/6‘1/6‘1/6‘l/6‘

We have already found the mean to be ll = 5, therefore, the variance is
given by
aﬁ = <0-s)2(éj+(20-32%)+(0-5)2[%)+<40-5)2(%j
+<0-5)2[%)+ (-30-92%] = ? = 4ss,33§
The standard deviation can be found by simply taking the square root of
the variance, Therefore, the standard deviation is
(TX = x/458.33g = 2140872096

Notice that if X has cenain dimensions or units, such
as centimeters (cm), then the variance ofX has units cmz .
while the standard deviation has the same unit as X, i.e., 
cm. It is for this reason that the standard deviation is
_
often used. 


%5 30 PROBABILITY AND STATISTICS
%============================================================================================================================================== %

Some Theorems on Expectation
Theorem 3-1: If c is any constant, then
E(cX) = cE(X) (U)
Theorem 3-2: IfX and Y are any random variables, then
E(X+Y):E(X)+E(Y) (12)
Theorem 3-3: IfX and Yare independent random variables, then
E(XY)=E(X)E(Y) (13)
* Note!

These properties hold for any random variable, notjust dis-
crete random variables. We will examine another type of
random variable in the next chapter.
Some Theorems on Variance

Theorem 3-4:
62 = E[(X—u)21= E<X’>—u’ = E<XZ>—rE<X>12 <14>
where y : E(X).

%%-CHAPTER 3; Discrete Random Variables 31

\subsection{Theorem 3-5:} If c is any constant,
Var(cX) = c2Var(X) (15)
Theorem 3-6: The quantity E[(X — a)2] is a minimum when (16)
1' = /1 = E(X)
Theorem 3-7: If X and Y are independent random variables,
Var(X+ Y) = vmx) + Var(Y) or o§+, = (:1 +01 (17)
Var(X - Y) = Var(X) + Var(Y) or <s§_,, = o§ +<s§

%============================================================================================================================================== %
Don’t Forget
These theorems apply to the variance and not to the standard devi-
ation! Make sure you convert your
standard deviation into variance
before you apply these theorems.
%============================================================================================================================================== %

Generalizations of Theorem 3-7 to more than two independent ran-
dom variables are easily made‘ In words, the variance ofa sum ofinde-
pendent variables equals the sum of their variances.
Again, these theorems hold true for discrete and nondiscrete ran-
dom variables.

%============================================================================================================================================== %


% %- 32 PROBABILITY AND STATISTICS
\subsubection*{Example 3.4.} 
Let X and Y be the random independent events of
rolling a fair die. Compute the expected value of X + Y, and the variance
of X + Y.

The following is the probability function for X and Y, individually:
lX,l1l1l3l4l5l6l
‘ﬂxj)‘1/6 l 1/6l1/6‘1/6‘ 1/6l1/6‘

From this, we get the following:
“X = ii,= 345 and o'§ = vi = 2.91666

There are two ways We could compute E(X + Y) and Var(X + Y).

%============================================================================================================================================== %

First, we could compute the probability distribution of X + Y, and ﬁnd
the expected value and variance from there. Notice that the possible val-
ues forX+ Yare 2, 3,  ll, 12.
E
ix+y iv is l9 lio in lizl
lﬂx +y) l 6/36 ls/36 l 4/36 l 3/36 l2/36‘ 1/36‘

We can ﬁnd the expected value as follows:
1 2 2 l 252
EX Y:2— 3— 1l— l2—:—:7
<~ <>(36)+<>(36)+ ~ W to 36

It then follows that the variance is:

Var(X+ Y) : [(2-vffija,"<12-vfféﬂ = % = 5.s33§



% % - CHAPTER 3; Discrete Random Variables 33
However, using Theorems 3-2 and 3-7 makes this an easy task.
By using Theorem 3-2,
\[E(X+ Y) = E(X) + E(Y) = 3.5 + 3.5 = 7.\]
By using Theorem 3-7,
Var(X+ Y) = Var(X)+ Var(Y) = 2.91666+2.91666 = 5A833§
Since X : Y, we could have also found the expected value using
Theorems 3-1:

\[E(X + Y) = E(X + X) = E(2X) = 2[E(X)] = 2(3,5) = 7\]
However, we could not have used Theorem 3-5 to ﬁnd the variance
because we are basically using the same distribution, X, twice, and X is
not independent from itself. Notice that we get the wrong variance when
we apply the theorem:
\[Var(X + X) = vmzx) = (2Z)Var(X) = 4Var(X) = 11,666\]

%============================================================================================================================================== %
