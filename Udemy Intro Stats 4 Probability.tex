\documentclass[12pt]{article}

%opening
\title{Introduction to Statistics and Probability}
\author{Kevin O'Brien}

\begin{document}

\maketitle
\newpage
\tableofcontents

% Section 1 Introduction to Statistics
% Section Descriptive Statistics
% Graphical Techniques
% Counting
% Introduction to Probability
% Discrete RVs
% Binomial Distribution

%--------------------------------------------------------------%
\newpage
\subsection{Interpreting Box-plots}

\begin{itemize} 
\item \textbf{Outliers}\\
The first feature that you look for when analysing a boxplot is the presence of outliers.
 
Outliers are extreme values and can greatly influence your analysis. For that reason, you should check your data and make sure you have entered it correctly.
 
You also have the option of removing outliers, making a note that you have removed them, and presenting your analysis without them.
 
\item \textbf{Skewed Data} \\
The second feature is the degree of skewness. As you learned earlier, the quartiles divide the data into four sections, each containg 25\% of the measurements. 

You are interested in how spread out or tightly packed the data are. The length of the whiskers and the position of the median in the box tell you this. Notice that 25\% of the values in the boxplot are less than Q1 and this includes the outliers.


\item \textbf{IQR} \\
The third feature is the variation/dispersion around the median. The IQR is the middle 50\% of the data. When you are dealing with skewed data, the IQR is the most reliable measure of variation. Outliers affect the mean, making it an unrealistic measure of centrality (for symmetric data).

The most common use of box plots is for comparing two data sets on the same scale. 

For now, it is important that you are clear what a box-plot tells you about a distribution of data and what measure of centrality and variability are most appropriate based on the distribution.

\end{itemize} 

%----------------------------------------------------------------%

\newpage
\section{Probability}

\subsection{Introduction to Probability}

There are many situations in everyday life where the outcome is not known with certainty. For example; applying for a job or sitting an examination.

We use words like "Chance", "the odds", "likelihood" etc but the most effective way of dealing with uncertainty is based on the concept of probability.

Probability can be thought of as a number which measures the chance or likelihood that a particular event will occur.


An example of the use of probability is in decision making. Decision making usually involves uncertainty. For example, should we invest in a company if there is a chance it will fail? 

Should we start production of a product even though there is a likelihood that the raw materials will arrive on time in poor? Having a number which measures the chances of these events occurring helps us to make a decision.



Why are we interested in probability in this module? Many statistical methods use the idea of a probability distribution for this data.

We have already looked at relative frequency distribution in Section 2. Probability distributions are based on the same concepts as relative frequency distributions. They are used to calculate probabilities of different values occurring in the data collected.

We will examine probability distributions in more detail in Section 4. First we need to learn about the basic concepts of probability.


\subsection{Random experiment}
\begin{itemize}
\item \textbf{Sample Space}, S. For a given experiment the sample space, S, is the set of all
possible outcomes.
\item \textbf{Event}, E. This is a subset of S. If an event E occurs, the outcome of the experiment is contained in E.
\end{itemize}

Probability concerns itself with random phenomena or probability experiments. These experiments are all different in nature, and can concern things as diverse as rolling dice or flipping coins. The common thread that runs throughout these probability experiments is that there are observable outcomes. If we collect all of the possible outcomes together, then this forms a set that is known as the sample space.

In this set theory formulation of probability the sample space for a problem corresponds to an important set. Since the sample space contains every outcome that is possible, it forms a setting of everything that we can consider. So the sample space becomes the universal set in use for a particular probability experiment.

A probability distribution is a table of values showing the probabilities of various outcomes of an experiment.

For example, if a coin is tossed three times, the number of heads obtained can be 0, 1, 2 or 3. The probabilities of each of these possibilities can be tabulated as shown:

\begin{tabular}{|c|c|c|c|c|}
\hline Number of Heads & 0 & 1 & 2 & 3 \\ 
\hline Probability & 1/8  & 3/8  & 3/8 & 1/8 \\ 
\hline 
\end{tabular} 

A discrete variable is a variable which can only take a countable number of values. In this example, the number of heads can only take 4 values (0, 1, 2, 3) and so the variable is discrete. The variable is said to be random if the sum of the probabilities is one. 


%--------------------------------------------------------------- %
\subsubsection{Common Sample Spaces}

Sample spaces abound and are infinite in number. But there are a few that are frequently used for examples in introductory statistics. Below are the experiments and their corresponding sample spaces:

\begin{itemize}
\item For the experiment of flipping a coin, the sample space is {Heads, Tails} and has two elements.

\item For the experiment of flipping two coins, the sample space is {(Heads, Heads), (Heads, Tails), (Tails, Heads), (Tails, Tails) } and has four elements.

\item For the experiment of flipping three coins, the sample space is {(Heads, Heads, Heads), (Heads, Heads, Tails), (Heads, Tails, Heads), (Heads, Tails, Tails), (Tails, Heads, Heads), (Tails, Heads, Tails), (Tails, Tails, Heads), (Tails, Tails, Tails) } and has eight elements.

\item For the experiment of flipping n coins, where n is a positive whole number, the sample space consists of 2n elements. There are a total of $C(n, k)$ ways to obtain k heads and $n - k$ tails for each number k from 0 to n.

\item For the experiment consisting of rolling a single six-sided die, the sample space is 
\[\{1, 2, 3, 4, 5, 6\} \]
\item For the experiment of rolling two six-sided dice, the sample space consists of the set of the 36 possible pairings of the numbers 1, 2, 3, 4, 5 and 6.
\item For the experiment of rolling three six-sided dice, the sample space consists of the set of the 216 possible triples of the numbers 1, 2, 3, 4, 5 and 6.
\item For an experiment of drawing from a standard deck of cards, the sample space is the set that lists all 52 cards in a deck. For this example the sample space could only consider certain features of the cards, such as rank or suit.
\end{itemize}

\subsubsection{Forming Other Sample Spaces}

These are the basic sample spaces. Others are out there for different experiments. It is also possible to combine several of the above experiments. When this is done, we end up with a sample space that is the Cartesian product of our individual sample spaces. We can also use a tree diagram to form these sample spaces.


\subsection*{What is a contingency table?}

A contingency table is essentially a display format used to analyse and record the relationship between two or more categorical variables. It is the categorical equivalent of the scatterplot used to analyse the relationship between two continuous variables.

\subsection{Combining Probabilities}

Events rarely occur in isolation. Usually we are interested in a combination or compound of events; for example
\begin{itemize}
\item The probability that two sections of a factory will be understaffed on the same day 
\item The probability of having a car accident today, given that you have had a car accident in the last five years.
\end{itemize}	

We will look at two laws of probability for combining events
\begin{itemize}
\item The Addition Law 
\item The multiplication Law
\end{itemize}	


\subsection{Conditional Probability}
The conditional probability of an event is the probability that an event A occurs given that another event B has already occurred. This type of probability is calculated by restricting the sample space that we’re working with to only the set B.

The formula for conditional probability can be rewritten using some basic algebra. Instead of the formula:

\[P(A | B) = \frac{P(A \cap B) }{P( B )}  \]

\subsection{Probability trees}
The setting out of solutions to problems requiring the manipulation of the probabilities of mutually exclusive and independent events can sometimes be helped by the use of probability tree diagrams. These have useful applications in decision theory.

The best choice of probability tree structure often depends upon the question and the natural order in which events like A and B above occur.


\subsection{Histograms}
A histogram is constructed from a frequency table. The intervals are shown on the X-axis and the number of scores in each interval is represented by the height of a rectangle located above the interval. A histogram of the response times from the dataset Target RT is shown below.



\subsection{Cumulative Distribution Function}

The cumulative distribution function (c.d.f.) of a discrete random variable X is the function F(t) which tells you the probability that X is less than or equal to t. So if X has p.d.f. P(X = x), we have:

\[F(t) = P(X \leq 1)\] % = SP(X = x).

In other words, for each value that X can be which is less than or equal to t, work out the probability that X is that value and add up all such results.

\textbf{Example}\\

In the above example where the die is thrown repeatedly, lets work out $P(X \leq t)$ for some values of t.

P(X $\leq$ 1) is the probability that the number of throws until we get a 6 is less than or equal to 1. So it is either 0 or 1. 

\begin{itemize}
\item P(X = 0) = 0 
\item $P(X = 1) = 1/6$.
\item  Hence $P(X \leq 1) = 1/6$
\end{itemize}

Similarly, $P(X \leq 2) = P(X = 0) + P(X = 1) + P(X = 2)$\\ = 0 + 1/6 + 5/36 = 11/36

%--------------------------------------------------------------%
\newpage
\section{Techniques for Counting}

\begin{itemize}
\item Combinations
\item Permutations
\item Permutations with constraints
\end{itemize}



\subsection{Permuations of subsets}

The number of permutations of subsets of $k$ elements selected from a set of $n$ different elements is

\[P(n,r) = \frac{n!}{(n-k)!}  \]


\subsection{Combinations of subsets}

The number of combinations that can be selected from $n$ items is

\[ {n \choose k} = \frac{n!}{k! \times (n-k)!}  \]
%--------------------------------------------------------------%
\newpage
\section{Discrete Random Variables}

A random variable is a numerical description of the outcome of an experiment.

Random variables can be classified as discrete or continuous, depending on the numerical values they may take.

A ranom variable that may assume any numerical value in an interval or collection of intervals is called a continuous random variable.


%--------------------------------------------------------------%
\newpage
\section{Discrete Probability Distributions}

\begin{itemize}
\item Poisson
\item Binomial
\item Geometric
\end{itemize}


\subsection{What Is a Probability Distribution?}
If you spend much time at all dealing with statistics, pretty soon you run into the phrase “probability distribution.” It is here that we really get to see how much the areas of probability and statistics overlap. Although this may sound like something technical, the phrase probability distribution is really just a way to talk about organizing a list of probabilities. A probability distribution is a function or rule that assigns probabilities to each value of a random variable. The distribution may in some cases be listed. In other cases it is presented as a graph.


\subsubsection{Graph of a Probability Distribution}

A probability distribution can be graphed, and sometimes this helps to show us features of the distribution that were not apparent from just reading the list of probabilities. The random variable is plotted along the x-axis, and the corresponding probability is plotted along the y - axis.

\begin{itemize}
\item For a discrete random variable, we will have a histogram
\item For a continuous random variable, we will have the inside of a smooth curve
\end{itemize}

The rules of probability are still in effect, and they manifest themselves in a few ways. Since probabilities are greater than or equal to zero, the graph of a probability distribution must have y-coordinates that are nonnegative. Another feature of probabilities, namely that one is the maximum that the probability of an event can be, shows up in another way.

\[ \mbox{Area} = \mbox{Probability} \]



\newpage
\section{Binomial Probability Distribution}
The binomial distribution is a particular example of a probability distribution involving a discrete random variable. 
It is important that you can identify situations which can be modelled using the binomial distribution. 
\begin{itemize}
\item There are n independent trials 
\item There are just two possible outcomes to each trial, success and failure, with fixed probabilities of p and q respectively, where q = 1 – p. 
\end{itemize}

The discrete random variable X is the number of successes in the n trials. 
$X$ is modelled by the binomial distribution $B(n,p)$. You can write $X \sim B(n, p)$.



\subsection{Poisson probability distribution}

A discrete random variable that is often used is one which estimates the number of occurrences  over a specified time period or space.

(remark : a specified space can be a specified length , a specified area, or a specified volume.)

If the following two properties are satisfied, the number of occurrences is a random variable described by the Poisson probability distribution

\textbf{Properties}\\
1)      The probability of an occurrence is the same for any two intervals of equal length.\\
2)      The occurrence or non-occurrence in any interval is independent of the occurrence or non-occurrence in any other interval.\\

The Poisson probability function is given by

 
 
\begin{itemize} 
\item	f(x) the probability of x occurrences in an interval. 
\item	$\lambda$ is the expected value of the mean number of occurrences in any interval. (We often call this the Poisson mean)
\item	e=2.71828284
\end{itemize} 

\subsection{Poisson Approximation of the Binomial Probability Distribution}

The Poisson distribution can be used  as an approximation of the binomial probability distribution when p, the probability of success is small and n, the number of trials is large.
We set   (other notation  )  and use the Poisson tables. 
 
As a rule of thumb, the approximation will be good wherever both  and  
 

%--------------------------------------------------------------------------%
\newpage
\section{Normal Probability Distribution}

\subsection{Bell Curve}
Bell curves show up throughout statistics. Diverse measurements such as diameters of seeds, lengths of fish fins, scores on the SAT and weights of individual sheets of a ream of paper all form bell curves when they are graphed. The general shape of all of these curves is the same. But all of these curves are different, because it is highly unlikely that any of them share the same mean or standard deviation. Bell curves with large standard deviations are wide, and bell curves with small standard deviations are skinny. Bell curves with larger means are shifted more to the right than those with smaller means.

\subsection*{Characteristics of the Normal probability distribution}

\begin{enumerate}
\item The highest point on the normal curve is at the mean, which is also the median and mode of the distribution.

\item The normal probability curve is bell-shaped and symmetric, with the shape of the curve to the left of the mean a mirror image of the shape of the curve to the right of the mean.

\item The standard deviation determines the width of the curve. Larger values of the the standard deviation result in wider flatter curves, showing more dispersion in data.

\item The total area under the curve for the normal probability distribution is 1.
\end{enumerate}

\end{document}
