
Chapter 8
TEST OF
HYPOTHESIS AND
SIGNIFICANCE
IN THIS CHAPTERI
V Statistical Decisions
V Statistical Hypothesis
V Tests of Hypothesis and Signiﬁcance
V Type I and Type ll Errors
V Level of Signiﬁcance
V Test Involving the Normal Distribution
V One-Tailed and Two-Tailed Tests
V P Value
V Special Tests
V Relationship between Estimation
Theory and Hypothesis Testing
Statistical Decisions
Very often in practice we are called upon to make decisions about pOp—
ulations on the basis of sample information. Such decisions are called
%%85
%%Copyright 2001 by the McGIaW-Hill Companies, Inc. Click Hem for Terms of Use.
%%-86 PROBABILITY AND STATISTICS
statistical decisions. For example, we may wish to decide on the basis
of sample data whether a new serum is really effective in curing a dis-
ease, Whether one educational procedure is better than another, or whether a given coin is loaded.
\section{Statistical Hypothesis}
In attempting to reach decisions, it is useful to make assumptions or
guesses about the populations involved. Such assumptions, which may
or may not be true, are called statistical hypotheses and in general are
statements about the probability distributions of the populations.
For example, if we want to decide whether a given coin is loaded, we formulate the
hypothesis that the coin is fair, i.e., p = 0.5, 

where p is the probability of heads. Similarly, if We want to decide Whether one procedure isbetter than another, we formulate the hypothesis that there is no diﬂlerence between the two procedures (i,e., any observed differences are
merely due to ﬂuctuations in sampling from the same population). Such hypotheses are often called null hypotheses, denoted by H0.

Any hypothesis that differs from a given null hypothesis is called
an alternative hypothesis. For example, if the null hypothesis is p = 0,5,
possible alternative hypotheses arep = 0.7, p ¢ 0.5, orp > 0.5. Ahypoth-
esis alternative to the null hypothesis is denoted by HI.
\subsection{Tests of Hypothesis and Significance}
If on the supposition that a particular hypothesis is true we ﬁnd that
results observed in a random sample differ markedly from those expect-
ed under the hypothesis on the basis of pure chance using sampling the-
ory, we would say that the observed differences are signiﬁcant a.nd we

%%-CHAPTER 8: Test of Hypothesis and Significance 87
would be inclined to reject the hypothesis (or at least not accept it on the
basis of the evidence obtained). For example, if 20 tosses of a coin yield 16 heads, we would be inclined to reject the hypothesis that the coin is fair, although it is conceivable that we might be wrong.
\subsection{You Need to Know}
Procedures that enable us to decide whether to accept or reject hypothesis or to determine whether observed samples
differ signiﬁcantly from expected results are called tests of
hypotheses, tests of signiﬁcance, or decision rules.
\section{Type I and Type ll Errors}
If we reject a hypothesis when it happens to be true, We say that a Tvpe
I error has been made, If, on the other hand, we accept a hypothesis
when it should be rejected, we say that a Type II error has been made.
In either case a wrong decision or error in judgment has occurred, In order for any tests of hypotheses or decision rules to be good,
they must be designed so as to minimize errors of decision. This is not
a simple matter since, for a given sample size, an attempt to decrease one type of error is accompanied in general by an increase in the other
type of error. In practice one type of error may be more serious than the other, and so a compromise should be reached in favor of a limitation of the more serious error. The only way to reduce both types of errors is to increase the sample size, which may or may not be possible.

\subsubsection{Level of Significance}
In testing a given hypothesis, the maximum probability with which we would be willing to risk a Type I error is called the level of significance of the test. This probability is often speciﬁed before any samples are drawn so that results obtained will not inﬂuence our decision.

%%-88 PROBABILITY AND STATISTICS
In practice the level of signiﬁcance of 0.05 or 0.01 is customary,
although other values are used. If for example a 0.05 or 5\% level of signiﬁcance is chosen in designing a test of a hypothesis, then there are about 5 chances in 100 that we would reject the hypothesis when it should be accepted; i.e., whenever the null hypothesis is true, we are about 95\% conﬁdent that we would make the right decision. In such
cases we say that the hypothesis has been rejected at a 0.05 level of significunce, which means that we could be wrong with probability 0.05.
* Note!
Choosing your level of signiﬁcance before you begin testing
will greatly aid you in choosing whether to accept or reject a
null hypothesis.
%==============================================================%
\section{Test Involving the Normal Distribution}
To illustrate the ideas presented above, suppose that under a given
hypothesis, the sampling distribution of a statistic S is a normal distri-
bution with mean us and standard deviation GS. The distribution of that
standard variable Z = (S — ps)/0's is the standard normal distribution
(mean O, variance l) shown in Figure 8~l, and extreme values of Z
would lead to the rejection of the hypothesis.
Critical ; ; Critical
region ‘ I region
: -' 4.96 : - 1.96
Figure 8-1



%%- CHAPTER 8: Test of Hypothesis and Significance 89
As indicated in the ﬁgure, we can be 95\% conﬁdent that, if the
hypothesis is true, the z score of an actual sample statistic S will be
between -1.96 and l.96 (since the area under the normal curve between
these values is 0.95).

However, if on choosing a single sample at random we ﬁnd that the z score of its statistic lies /
outside the range —l.96 to 1.96, we would con- ' ‘i K
clude that such an event could happen with the ‘vb 
probability of only 0.05 (total shaded area in the ﬁgure) if the given hypothesis was tme. We would then say that this z score differed significantly from
what would be expected under the hypothesis, and we would be inclined to reject the hypothesis.
The total shaded area 0.05 is the level of signiﬁcance of the test. 

It represents the probability of our being Wrong in rejecting the hypothesis, i.e., the probability of making a Type I error. Therefore, we say that the hypothesis is rejected at a 0.05 level of signiﬁcance or that the z
score of the given sample statistic is signiﬁcant at a 0.05 level of significance.

The set ofz scores outside the range —l .96 to 1.96 constitutes what
is called the critical region or region of rejection of the hypothesis or the
region ofsignificance. The set of z scores inside the range —l .96 to 1.96
could then be called the region of acceptance of the hypothesis or the
region of nonsigniﬁcance.
On the basis of the above remarks, we can formulate the following
decision rule:
(a) Reject the hypothesis at a 0.05 level of signiﬁcance if the z
score of the statistic S lies outside the range —l .96 to 1.96 (i.e.,
if either Z > 1.96 or Z < ~l.96). This is equivalent to saying that
the observed sample statistic is signiﬁcant at the 0.05 level.
(b) Accept the hypothesis (or, if desired, make no decision at all)
otherwise.
It should be noted that other levels of signiﬁcance could have been
used. For example, if a 0.01 level were used we would replace 1.96
everywhere above by 2.58 (see Table 8.1). Table 7.1 can also be used
since the sum of the level of signiﬁcance and level of conﬁdence is
100%.


%===========================================================================%
%%- 90 PROBABILITY AND STATISTICS
\subsection{One-Tailed and Two-Tailed Tests}
In the above test we displayed interest in extreme values of the statistic
S or its corresponding z score on both sides of the mean, i.e., in both
tails of the distribution. For this reason such tests are called two-tailed
tests or two-sided tests.
Often, however, we may be interested only in extreme values
to one side of the mean, i.e., in one tail of the distribution, as for exam-
ple, when we are testing the hypothesis that one process is better that
another (which is different from testing whether one process is better or
Worse than the other). Such tests are called one-tailed tests or one-sided
tests. In such cases the critical region is a region to one side of the dis-
tribution, with area equal to the level of signiﬁcance.
Table 8.1, which gives values of Z for both one—tailed and two-tailed tests at various levels of signiﬁcance, will be useful for reference
purposes. Critical values of z for other levels of signiﬁcance are found
by use of the table of normal cuwe areas.
Table 8-1
Level of Signiﬁanoe u 0.10 0.05 0.01 I 0.(X)$
Cnncal Vl|lI5 0|" : for -L28 -L645 -2.33 \ -2.58
One-Tailed Tau or 1.28 I or L645 i or 2.33 or 2.58
(‘nncal Values cl‘: for -L645 -L96 -2.58 -2.81
Two-Tailed Tests and L645 and L96 and 2.58 and 2.8]
P Value
In most of the tests we will consider. the null hypothesis HO will be an
assertion that a population parameter has a speciﬁc value, and the alter-
native hypothesis H, will be one of the following two assertions:
(i) The parameter is greater than the stated value (right-tailed
test).
%================================================================================%
%---CHAPTER 8: Test of Hypothesis and Significance 91
(ii) The parameter is less that the stated value (left-tailed test).
(m) The parameter is either greater than or less than the stated
value (two~tailed test).
In Cases (i) and (ii), H] has a single direction with respect to the
parameter, and in case (m), H ‘ is bi-directional After the test has been
performed and the test statistic S computed, the P value of the test is the
probability that a value of S in the direction(s) of H1 and as extreme as
the one that actually did occur if H“ were true,
For example, suppose the standard deviation (S of a normal popula-
tion is known to be 3, and H0 asserts that the mean u is equal to 12. A
random sample of size 36 drawn from the population yields a sample
mean Z = 1295. The test statistic is chosen to be
Z:X—l2:X—l2’
on/Z 0,5
which, if H“ is true, is the standard normal variable. The test value of Z
is the following:
Z: 1295-12 :19‘
0.5
The P value for the test then depends on the alternative hypothesis HI
as follows:
(i) For H1: u > l2 [case (i) above], the P value is the probability
that a random sample of size 36 would yield a sample mean of
12.95 or more if the true mean were 12, i.e., P(Z Z 19) = 0.029.
In other words, the chances are about 3 in 100 that 2? 2 12.95
if 1.1 = 12;
(ii) For HI: |J. < 12 [case (ii) above], the P value is the probability
that a random sample of size 36 would yield a sample mean of

%--------------------------------------------------------------------------------------%
%%- 92 PROBABILITY AND STATISTICS
12.95 or less if the true mean were 12, i.e., P(Z £ 19) = 0.971.
In other words, the chances are about 97 in 100 that I 5 12.95
if u = 12.
(m) For HI: pt ¢ 12 [case (m) above], the P value is the probability
that a random sample mean 0.95 or more units away from 12,
i.e., Y 2 12.95 or)? S 11.05, if the true mean were 12. Here the
P value is P(Z 2 19) + P(Z S -19) = 0.057, which says the
chances are about 6 in 100 that Ir - 12| 2 0.095 if |.t = 12.

Small P values provide evidence for rejecting the null hypothesis in
favor of the alternative hypothesis, and large P values provide evidence
for not rejecting the null hypothesis in favor of the alternative hypothe-
sis. In case (i) of the above example, the small P value 0.029 is a fairly
strong indicator that the population mean is greater than 12, whereas in
case (ii), the large P value 0.971 strongly suggests that H0 jll = 12
should not be rejected in favor of H0 :# < 12. In case (m), the P value
0.057 provides evidence for rejecting H0 in favor of HO : [1 ¢ 12 but not
as much evidence as is provided for rejecting H0 in favor of H0 : /,1 > 12.
It should be kept in mind that the P value and the level of signiﬁ-
cance do not provide criteria for rejecting or not rejecting the null
hypothesis by itself, but for rejecting or not rejecting the null hypothe-
sis in favor of the alternative hypothesis. As the previous example illus-
trates, identical test results and different signiﬁcance levels can lead to
different conclusions regarding the same null hypothesis in relation to
different alternative hypothesis.
When the test statistic S is the standard normal random variable, the
table in Appendix B is sufﬁcient to compute the P value, but when S is
one of the t, F , or chi-square random variables, all of which have dif-
ferent distributions depending on their degrees of freedom, either com-
puter software or more extensive tables than those in Appendices C, D,
and E will be needed to compute the P value.
Example 8.1. The mean lifetime of a sample of 100 ﬂuorescent
light bulbs produced by a company is computed to be 1570 hours with
a standard deviation of 120 hours. If pt is the mean lifetime of all the
bulbs produced by the company, test the hypothesis 11 : 1600 hours



CHAPTER 8: Test of Hypothesis and Significance 93
against the alternative hypothesis ll at 1600 hours. Use a signiﬁcance
level of 0.05 and ﬁnd the P value of the test.
We must decide between the two hypotheses
HO:/1 = 1600 hours H0 :,u ¢ 1600 hours
A two—tailed test should be used here since ,u ¢ 1600 includes both val-
ues large and smaller than 1600.
For a two-tailed test at a level of signiﬁcance of 0.05, we have the
following decision rule:
l. Reject H0 if the z score of the sample mean is outside the range
-1.96 to 1.96.
2. Accept HU (or withhold any decision) otherwise.
The statistic under consideration is the sample mean Y. The sam-
pling distribution of X has a mean /,1); = /,1 and standard deviation
0'; = 6 / W , where u and 6 are the mean and standard deviation of the
population of all bulbs produced by the company.
Under the hypothesis HO, we have ,u = 1600 and 6)? = 0' / \/Z = 120/
M =12, using the sample standard deviation as an estimate of 6.
Since Z = (.3? — 1600)/l2 = (1570 — 1600)/l2 = -2.50 lies outside the
range -1.96 to 1.96, we reject H0 at a 0.05 level of signiﬁcance.
The P value of the two tailed test is P(Z é -2.50) + P(Z 2 2.50) =
0.0|24, which is the probability that a mean lifetime of less than 1570
hours or more than l63O hours would occur by chance if H0 were true.
Special Tests
For large samples, many statistics S have nearly normal distributions
with mean us and standard deviation GS. ln such cases we can use the
above results to formulate decision rules or tests of hypotheses and sig-
niﬁcance. The following special cases are just a few of the statistics of



94 PROBABILITY AND STATISTICS
practical interest. In each case the results hold for inﬁnite populations or
for sampling with replacement. For sampling without replacement from
ﬁnite populations, the results must be modiﬁed. We shall only consider
the cases for large samples (n 2 30).
1. Means. Here S = Y, the sample mean; /.13 = '11); = pt, the popu-
lation mean; GS =0‘)? =0‘/x/Z, where G is the population
standard deviation and n is the sample size, The standardized
variable is given by
_ X-/1
21;/./; U)
When necessary the observed sample standard deviation, .r (or
5), is used to estimate 6.
To test the null hypothesis HU that the population
mean is ,u = a, we would use the statistic (1). Then, if the alter-
native hypothesis is /1 = a, using a two-tailed test, we would
accept H0 (or at least not reject it) at the 0.05 level if for a par-
ticular sample of size n having mean )7
-1.9631996 (2)
0'/~/Z
and would reject it otherwise. For other signiﬁcance levels we
would change (2) appropriately. To test HO against the alterna-
tive hypothesis that the population mean is greater than a, we
would use a one-tailed test and accept H0 (or at least not reject
it) at the 0.05 level if
J? — a
— 1.645 3
O‘/\/;< ()



CHAPTER 8: Test of Hypothesis and Significance 95
(see Table 8.1) and reject it otherwise. To test H0 against the
alternative hypothesis that the population mean is less than a,
We Would accept HO at the 0.05 level if
i—a
7 L645 4
G/W> ()
Proportions Here S = P, the proportion of “successes” in
a sample; ‘us : pp : P, where p is the population proportion of
successes and n is the sample size; 0'5 = 0'}, = 1 pq/n , where
q = l —p. The standardized variable is given by
Zi (5)
1/pq/n
In case P = X/n, where X is the actual number of suc-
cesses in a sample, (5) becomes
X—np
Z= T (6)
\/"Pq
Remarks similar to those made above about one- and two—tailed tests for means can be made.

\subsection{Differences of Means}
Let Y I and Y2 be the sample means
obtained in large samples of sizes nl and n2 drawn from respec-
tive populations having means |.tl and pg and standard devia-
tions 61 and 0'2. Consider the null hypothesis that there is no
diﬂerence between the population means, i.e,, pl = pl. From
our discussion on the sampling distributions of differences and
sums (Chapter 6), on placing pt] = 112 We see that the sampling
distribution of differences in means is approximately normal
with mean and standard deviation given by


96 PROBABILITY AND STATISTICS
0 ‘J
0-“ 0-;
_ _ :0 ()'_ _ : _l+_—
'uX|—Xz X|—XZ "I "2 (7)
where we can, if necessary, use the observed sample standard
deviations sl and s2 (or 51 and §Z) as estimates of 6] and 61.
By using the standardized variable given by
Y —Y — Y —Y
kmoi (8)
63-2: “>?,_>?2
in a manner similar to that described in Part 1 above, we can
test the null hypothesis against an alternative hypothesis (or
the signiﬁcance of an observed difference) at an appropriate
level of signiﬁcance.
%===========================================================================================%
\section{4. Differences of Proportions} 
Let PI and P2 be the sample proportions obtained in large samples of sizes nl and nz
drawn from respective populations having proportions pl and p2. Consider the null hypothesis that there is no diﬂerence between the population proportions, i.e., pl = [12, and thus that the samples are really drawn from the same population,

From our discussions about the differences of proportions in Chapter 6, on placing 111 = [)2 = p, we see that the sam-
pling distribution of differences in proportions is approximate-
ly normal with mean and standard deviation given by
rm
l 1
/‘lP‘—P3 :0 GP,-P1 :\JiP(l—P)[Z+Zj (9)



CHAPTER 8: Test of Hypothesis and Significance 97
— P + P . . .
where P a IS used as an estimate of the popula-
n, + n2
tion proportion p.
By using the standardized variable
Z:Ft*Pr0:PrP2 (,0)
O-Pi’/”z O-PFP2
we can observe differences at an appropriate level of signiﬁ-
cance and thereby test the null hypothesis.
Tests involving other statistics can similarly be
designed.

\section{Relationship between Estimation}
\subsection{Theory and Hypothesis Testing}
From the above remarks one cannot help but notice that there is a rela-
tionship between estimation theory involving conﬁdence intervals and
the theory of hypothesis testing. For example, We note that the result (2)
for accepting H0 at the 0.05 level is equivalent to the result (1) in
Chapter 7, leading to the 95\% conﬁdence interval

Z_1.96o'<'u<}_1.96o
~/Z ’ ’ \/Z

Thus, at least in the case of two-tailed tests, We could actually
employ the conﬁdence intervals of Chapter 7 to test the hypothesis. A
similar result for one-tailed tests would require one-sided conﬁdence
intervals.

% 98 PROBABILITY AND STATISTICS\subsection{Example 8.2}
Consider Example 8.1. A 95\% conﬁdence interval for Example 8.1 is the following




(l.96)(l2O) (l.96)(l2O)
1570 — L S £1570 L
\/100 /I + x/lOO
which is
1570 — 23.52 S /1 S1570 + 23.52
This leads to an interval 0f(1546.48, 159352)‘ Notice that this
does not contain the alleged mean of 1600, thus leading us to reject H0.


