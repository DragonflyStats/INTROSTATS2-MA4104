

Basic Probability
Descriptive Statistics
Discrete Random Variables
Continuous Random Variables
Examples of Random Variables
Sampling Theory
%Estimation Theory
%Test of Hypothesis and
%Significance
%Curve Fitting, Regression,
%and Correlation
%Other Probability Distributions
%Mathematical Topics
%Areas under the Standard
%Normal Curve from 0 to z
%Student's t distribution
%Chi-Square Distribution
%95th and 99th Percentile Values
%for the F Distribution
%Values of e"*
%Random Numbers
%V



\section*{Chapter 1 BASIC PROBABILITY}

IN This CHAPTER:

\begin{enumerate}
\item Random Experiments
\item Sample Spaces
\item Events
\item The Concept of Probability
\item The Axioms of Probability
\item Some Important Theorems on Probability
\item Assignment of Probabilities
\item Conditional Probability
\item Theorem on Conditional Probability
\item Independent Events
\item Bayes’ Theorem or Rule
\item Combinatorial Analysis
\item Fundamental Principle of Counting
\item Permutations
\item Combinations
\end{enumerate}
\newpage


2 PROBABILITY AND STATISTICS
8/ Binomial Coefﬁcients
V Stirling’s Approximation to n!
%===========================================================================================%
\subsection*{Random Experiments}
We are all familiar with the importance of experi~
ments in science and engineering. Experimentation V K
is useful to us because we can assume that if we perform certain experiments under very nearly “J; 
identical conditions, we will arrive at results that
are essentially the same. In these circumstances,
we are able to control the value of the variables
that affect the outcome of the experiment.

However, in some experiments, we are not able to ascertain or con-
trol the value of certain variables so that the results will vary from one
performance of the experiment to the next, even though most of the con-
ditions are the same. These experiments are described as random. Here
is an example:

Example 1.1. If we toss a die, the result of the experiment is that it
will come up with one of the numbers in the set {l, 2, 3, 4, 5, 6}.

\subsection*{Sample Spaces}
A set S that consists of all possible outcomes of a random experiment is
called a sample space, and each outcome is called a sample p0l'l'it, Often
there will be more than one sample space that can describe outcomes of
an experiment, but there is usually only one that will provide the most
information.

\subsubsection*{Example 1.2.} 
If we toss a die, then one sample space is given by
[l, 2, 3, 4, 5, 6} while another is (even, odd}. It is clear, however, that
the latter would not be adequate to determine, for example, whether an
outcome is divisible by 3.
If is often useful to portray a sample space graphically, In such cases,
it is desirable to use numbers in place of letters whenever possible.



% % -- CHAPTER 1; Basic Probability 3
If a sample space has zi ﬁnite number of points, it is called a ﬁnite
sample space. If it has as many points as there are natural numbers 1, 2,
3,  , it is called a countably inﬁnite sample space. If it has as many
points as there are in some interval on the x axis, such as 0 3 x S 1, it is
called a noncountably inﬁnite sample space. A sample space that is
ﬁnite or countably ﬁnite is often called a discrete sample space, while
one that is noncountzibly inﬁnite is called a nnndiscrete sample space.
%------------------------------------------------------------------------------%
\subsection*{Example 1.3.} The sample space resulting from tossing a die yields
a discrete sample space. However, picking any number, not just inte-
gers, from 1 to 10, yields a nondiscrete sample space.
%============================================================================================================== %
\subsection*{Events}
An event is a subset A of the sample space S, i.e., it is a set of possible
outcomes. If the outcome of an experiment is an element of A, we say
that the event A has occurred. An event consisting of a single point of S
is called a simple or elementary event.
As particular events, we have S itself, which is the sure or certain
event since an element of S must occur, and the empty set (5, which is
called the impossible event because an element of Q cannot occur.
By using set operations on events in S, We can obtain other events
in S. For example, ifA and B are events, then
\begin{enumerate}
\item A L) B is the event “either A or B or both." A L) B is called the
union ofA and B.
\item A rw B is the event “both A and B." A H B is called the inter-
section of A and B.
\item A’ is the event “not A." A’ is called the complement ofA.
\item  A - B :A O B’ is the event “A but not B.” In particular, A’ :
S — A.
\end{enumerate}
If the sets corresponding to events A and B are disjoint, i.e., A rw B
= Q, We often say that the events are mutually exclusive. This means
that they cannot both occur. We say that a collection of events A I, A2,  ,
A" is mutually exclusive ifevery pair in the collection is mutually exclu-
sive.



% % - 4 PROBABILITY AND STATISTICS
\subsection*{The Concept of Probability}
In any random experiment there is always uncertainty as to whether a particular event will or will not occur. As a measure of the chance, or probability, with which we can expect the event to occur, it is convenient to assign a number between O and l. If we are sure or certain that an event will occur, we say that its probability is 100\% or 1. If we are
sure that the event will not occur, we say that its probability is zero. If, for example, the probability is 1/Z, we would say that there is a 25\% chance it will occur and a 75\% chance that it will not occur.
Equivalently, we can say that the odds against occurrence are 75\% to 25\%, or 3 to l.

There are two important procedures by means of which we can estimate the probability of an event.

%==========================================================%
\begin{description}
\item[CLASSICAL APPROACH:] If an event can occur in h
different ways out of a total of n possible ways, all of which
are equally likely, then the probability of the event is h/n.
\item[FREQUENCY APPROACH:] If after rt repetitions of an
experiment, where n is very large, an event is observed to
occur in h of these, then the probability of the event is I1/n. This
is also called the empirical probability of the event.
Both the classical and frequency approaches have serious drawbacks,
the ﬁrst because the words “equally likely” are vague and the second
because the “large number" involved is vague. Because of these difﬁculties,
mathematicians have been led to an axiomatic approach to probability.
\end{description}
%===============================================================%
\subsection*{The Axioms of Probability}
Suppose we have a sample space S. If S is discrete, all subsets correspond to events and Conversely; if S is nondiscrete, only special subsets (called measurable) correspond to events. To each event A in the class C of events, we associate a real number P(A). The P is called a probability function, and P(A) the probability of the event, if the following axioms are satisﬁed.

%=============================================================================================================== %

% %--- CHAPTER 1: Basic Probability 5
\begin{description}
\item[Axiom 1.] For every event A in class C,
P(A) 2 0
\item[Axiom 2.] For the sure or certain event S in the class C,
P(S) = l
\item[Axiom 3.] For any number of mutually exclusive events A1, Al, 
in the class C,
P(A] UA2 u  )= P(Al) +P(A2) + 
Ln particular, for two mutually exclusive events Al and A2 ,
P(A| U A2 ) = P(A,) + P(A2)
\end{description}

\subsection*{Some Important Theorems on Probability}
From the above axioms we can now prove various theorems on proba-
bility that are important in further Work.

%------------------------------------------------------------------------%
\begin{description}
\item[Theorem 1-1:] IfAl CA2 , then (I)
P(A,) S P(A2) and P(AZ — AI) = P(A1) — P(AZ)

\item[Theorem 1-2:] For every event A, (2)
0 5 P(A) 5 I,
i.e., a probability between O and l.

\item[Theorem 1-3:] For Q, the empty set, (3)
P(@) = 0
i.e,, the impossible event has probability zero.

\item[Theorem 1-4:] If A’ is the complement ofA, then (4)
P(A’) : l -P(A)

\item[Theorem 1-5:] IfA =A| u A2 u  u An , where ALA,‘  ‘An are
mutually exclusive events, then
\[P(A) = P(A1) + P(A2) +  + P(A")\] (5)
\item[Theorem 1-6:] If A and B are any two events, then (6)
\[P(A U B) : P(A) + P(B) - P(A A B)\]
\end{description}

%================================================================================================================ %
% % -6 PROBABILITY AND STATISTICS


More generally, if A1, AZ, A3 are any three events,
then
P(A] v AZ U A3) = P(A‘) + P(AZ) + P(A3) —
P(A‘ n AZ) - P(A2 n A3) - P(A} 0 Al) +
P(A! n A2 r\ A3).
%=============================================================================================================== %
Generalizations to n events can also be made,
Theorem 1-7: For any events A and B, (7)
P(A) = P(A Kw B) + P(A AB’)
%=============================================================================================================== %
\subsection{Assignment of Probabilities}
If a sample space S consists of a ﬁnite number of outcomes a,, a2,  ,
an, then by Theorem l~5,
P(A’) + P(A2) +  + P(A”) = 1 (8)
Where A], AZ,  , A" are elementary events given by A! = {ui},
It follows that we can arbitrarily choose any nonnegative numbers
for the probabilities of these simple events as long as the previous equa-
tion is satisﬁed. In particular, if we assume equal probabilities for all
simple events, then
\[ P(Ak)=;, k=l,2,...,n (9)\]

And if A is any event made up of h such simple events, We
have
P(A):g (10)
This is equivalent to the classical approach to probability. We could of course use other procedures for assigning probabilities, such as frequency approach.



%CHAPTER 1: Basic Probability 7
Assigning probabilities provides a mathematical model, the success
of which must be tested by experiment in much the same manner that
the theories in physics or others sciences must be tested by experiment
Remember: The probability for any event must be between 0 and 1.

%=============================================================================================================== %
\subsection*{Conditional Probability}
Let A and B be two events such that $P(A) > O$. Denote $P(B | A)$ the prob-
ability of B given that A has occurred Since A is known to have
occurred, it becomes the new sample space replacing the original S.
From this we are led to the deﬁnition
P(B|A)z  (11)
or
\[P(Ar\B)z P(A)P(B|A) \] %(12)
In words, this is saying that the probability that both A and B occur
is equal to the probability that A occurs times the probability that B
occurs given thatA has occurred We call P(B I A) the conditional prob-
ability of B given A, i.et, the probability that B will occur given that A
has occurredl It is easy to show that conditional probability satisﬁes the
axioms of probability previously discussed.

\subsection*{Theorem on Conditional Probability}
Theorem 1-8: For any three events A/, A2, A3, we have
\[P(A, r\A2 KWA3) = P(A,)P(A2 |A1)P(A3 I A] 0A2) (I3)]\




In words, the probability that A, and A2 and A3 all occur is equal
to the probability that A I occurs times the probability that A2 occurs
given that A1 has occurred times the probability that A3 occurs given
that both AI and A2 have occurred‘ The result is easily generalized to n
events.

\subsubsection*{Theorem 1-9:}
 If an event A must result in one of the mutually
exclusive events A, ,A2 ,  , A” , then \[P(A)
: P(A1)P(A|Al)+ P(AZ)P(A 1A2) +...
+ P(An)P(A IA”) \] % % (14)
%=========================================================================================== %
\subsection*{Independent Events}
If P(B I A) = P(B), i.e., the probability of B occurring is not affected by
the occurrence or nonoccurrence of A, then we say that A and B are
independent events. This is equivalent to
\[P(Ar\B): P(A)P(B) \] (15)
Notice also that if this equation holds, then A and B are indepen-
dent.
We say that three events A], A2, A3 are independent if they are
pairwise independent‘
P(A, O Ak) = P(A,)P(Ak)j ¢ k where j,k = 1,2,3 (16)
and
P(A| Kw A2 n A3) = P(A1)P(A2)P(A3) (17)
Both of these properties must hold in order for the events to be
independent. Independence of more than three events is easily
deﬁned.



CHAPTER 1; Basic Probability 9
* Note!
In order to use this multiplication rule, all of your events
must be independent.
%=======================================================================================%
\section{Bayes’ Theorem or Rule}
Suppose that A], A2,  , A" are mutually exclusive events whose union
is the sample space S, ikek, one of the events must occur. Then ifA is any
event, we have the important theorem:
Theorem 1-10 (Bayes’ Rule):
P A P AIA
P(Ak|A)=  (18)
2P(A/ )P(A | A/)
,=1
This enables us to ﬁnd the probabilities of the various events AI,
A2,  , A” that can occur, For this reason Bayes’ theorem is often
referred to as a theorem an the probability ofcemses.

\subsection*{Combinatorial Analysis}
In many cases the number of sample points in a sample space is not very large, and so
direct enumeration or counting of sample points needed to obtain probabilities is not
difﬁcult. However, problems arise where direct counting becomes a practical impos-
sibility. In such cases use is made of combinatorial analysis, which
could also be called a sophisticated way of counting.



% % 10 PROBABILITY AND STATISTICS
\subsection*{Fundamental Principle of Counting}
If one thing can be accomplished n] different ways and after this a sec-
ond thing can be accomplished n2 different Ways,  , and ﬁnally a kth
thing can be accomplished in nk different Ways, then all k things can be
accomplished in the speciﬁed order in nlnzmnk different Ways.
%=============================================================================================================== %
\subsection*{Permutations}
Suppose that We are given n distinct objects and Wish to arrange r of
these objects in a line. Since there are n ways of choosing the ﬁrst
object, and after this is done, n - 1 ways of choosing the second object,
 , and ﬁnally n — r + l ways of choosing the rth object, it follows by
the fundamental principle of counting that the number of different
arrangements, or permutations as they are often called, is given by
nP,=n(n—l),..(n—r+l) <19)
Where it is noted that the product has r factors. We call "Pr the number
ofpermutations ofn objects taken r at a time.
%=============================================================================================================== %

\subsection{Example 1.4.}
It is required to seat 5 men and 4 Women in a row so
that the Women occupy the even places. How many such arrangements are possible?
The men may be seated in SP5 ways, and the women 4P4 ways. Each arrangement of the men may be associated with each arrangement of the women. Hence, Number of arrangements = SP5, 4P4= 5! 4! = (l2O)(24) = 2880
In the particular case when r = n, this becomes
"R1 =n(n—l)(n—2)...l=nl (20)
which is called nfacturial. We can write this formula in terms of facto-
rials as

n!
n1=,=(n_r)! <21>
If r = n, we see that the two previous equations agree only if
we have 0! = 1, and we shall actually take this as the deﬁnition of 0!.

Suppose that a set consists of n objects of which rt] are of one
type (i.e., indistinguishable from each other), n2 are of a second type,  ,
nk are of a kth type. Here, of course, n=n] +n2 +"'+nk4 Then the
number of different permutations of the objects is
nl
,1 P = L (22)
n|,»|2t...,n‘ nl!n2!_4,nk!
%=============================================================================================================== %
\subsection*{Combinations}
In a permutation we are interested in the order of arrangements of the
objects. For example, abc is a different permutation from boa. In many
problems, however, we are only interested in selecting or choosing
objects without regard to order. Such selections are called combina-
tions. For example, abc and hca are the same combinations
The total number of combinations of r objects selected from n (also
called the combinations ofn things taken r at a time) is denoted by NC,‘
or  We have
r
("J = c. = L (23)
r V’ ' r! (n — r)!
It can also be Written
V1 n(n—l)--~(n—r+l) "P,
t1%:—. 
r. r.
It is easy to show that



12 PROBABILITY AND STATISTICS
Vl _ 7!
£rj—£n—rj or hcF:HCH—F (25)
%=============================================================================================================== %

Example 1.5. From 7 consonants and 5 vowels, how many words
can be formed consisting of 4 different consonants and 3 different vow-
els? The words need not have meaning‘
%=============================================================================================================== %

The four different consonants can be selected in 7C4 ways, the three dif-
ferent vowels can be selected in SC, Ways, and the resulting 7 different
letters can then be arranged among themselves in 7P7 : 7! ways. Then
Number of words = 7C4» 5C%< 7! = 35-10-5040 = 1,764,000

\subsection*{Binomial Coefﬁcients}
The numbers from the combinations formula are often called binomial
coeﬁicients because they arise in the binomial expansion
(X+y)n :Xn +[TjXn—ly+(ZJX»i—2y2 +___+£Yljyn (26)

%=============================================================================================================== %
\subsection{Stirling's Approximation to n!}
When n is large, a direct evaluation of n! may be impractical‘ In such
cases, use can be made of the approximate formula
n ~ \/21m n"e"' (Z7)
where e = 2.71828  , which is the base of natural logarithms. The
symbol ~ means that t_he ratio of the left side to the right side approach-
es l as n —> \><>.


% %- CHAPTER 1: Basic Probability 13
Computing technology has largely eclipsed the value of Stirling’s
formula for numerical computations, but the approximation remains
valuable for theoretical estimates (see Appendix A).

